---
title: 'INFO Blog 2: Harms in Machine Learning'
date: 2025-11-17
permalink: /posts/2025/10/ifo-blog-post-1/
tags:
  - AI ethics
  - Algorithmic harm
  - Machine learning bias
  - Data responsibility
  - Accountability in AI
  - Representation
  - Technology and society
---

*Exploring how AI systems turn people into data, and what it takes to bring empathy and accountability back into technology.* 

**Case Study:** 
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

This MIT SERC case study aims to map seven distinct ways harm can creep into machine learning systems. By tracing these risks across data collection, model development, evaluation and deployment, the authors argue that bias is not a single phenomenon but a *spectrum* that manifests differently at each stage.

## Breaking Down Bias: How Harms Emerge in the ML Life Cycle
When I first read the MIT article about sources of harm in the machine learning life cycle, I honestly didn’t expect it to make me reflect so much on how many small, invisible choices in tech design can have real social consequences. I used to think of bias in AI as just “bad data,” but this study broke it down into layers: historical, measurement, aggregation, learning, evaluation, and deployment bias, and it changed how I see technology altogether.

What struck me most was how ordinary some of these harms begin. For example, historical bias isn’t just about someone maliciously building an unfair system, it’s when a model reflects the world as it already is. The case with word embeddings really stayed with me: how words like “nurse” and “engineer” become gendered because of the data they’re trained on. It made me think of how much online language I consume every day and how invisible those patterns are until someone points them out.

Representation bias also hit close to home. The article described how datasets like ImageNet include mostly Western images, and as a result, models perform worse on people from other regions. I’ve actually noticed this myself: when I use translation apps, they sometimes glitch or misinterpret non Western features or accents. It made me realize how “global” tech is still built from a very specific lens, and how easy it is to forget that when you’re on the inside of a project or studying computer science.

Measurement bias made me think about my own academic life. The article gave an example about GPA being used to predict student success,it’s measurable, sure, but it completely misses other dimensions of growth like perseverance or creativity. I’ve been there, some semesters my GPA didn’t reflect how much I learned or improved, especially when I was balancing work and health. So when the authors said that using oversimplified measures leads to distorted results, I found myself nodding in full agreement.

Another part that stuck with me was learning bias, the idea that even how we optimize models can prioritize accuracy over fairness. I’ve seen that mindset even in small programming projects: we focus on metrics that look impressive, but rarely on who might be left out by those numbers.

What I appreciated most was the section on deployment bias. The idea that a model can work well in testing but still cause harm once it meets the real world felt uncomfortably true. The criminal justice risk assessment example was chilling it showed how an algorithm trained to “predict recidivism” could be misused in sentencing decisions. It reminded me that once a tool leaves the lab, it takes on a life of its own, often in systems that already have inequalities baked in.

## The Quetion I'd Ask 
One question I kept coming back to while reading was:
**How can we design systems that allow communities most affected by algorithmic decisions to participate in the design or auditing process itself?**
I included this question because the article made me realize that most harms aren’t purely technical, they’re social. If the people who experience the consequences aren’t part of the process, the harm just cycles back.

## Final Thoughts
Writing this blog post made me notice how often we (especially students in computer science) talk about “building things that work,” but not enough about how they work for different people. The article didn’t just teach me vocabulary like “aggregation bias” or “evaluation bias”, it gave me a new lens to look at responsibility. I don’t think the point is to never make mistakes in ML, but to stay aware that our decisions, even seemingly small ones, can shape real outcomes for others.