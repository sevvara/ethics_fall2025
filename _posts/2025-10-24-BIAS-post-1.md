---
title: 'BIAS Blog 1: AI’s Regimes of Representation'
date: 2025-10-24
permalink: /posts/2025/10/BIAS-blog-post-1/
tags:
  - AI ethics
  - AI bias 
  - Cultures
  - AI represents
  - text-to-image models 
  - DALL·E
  - Stable Diffusion
---

*Questioning how AI reduces human complexity into data, and how ethics helps us confront what gets lost in that process.* 

**Case Study:** 
[AI’s Regimes of Representation: A Community-Centered Study of Text-to-Image Models in South Asia](https://mit-serc.pubpub.org/pub/bfw5tscj/release/3?readingCollection=65a1a268)


## Right to Fair Representation: Reflections on AI’s Regimes of Representation

When I first read AI’s Regimes of Representation, I didn’t expect it to make me question my own relationship with technology so deeply. The study explored how text-to-image AI models like DALL·E or Stable Diffusion portray people from South Asia and, more importantly, what happens when a machine becomes the storyteller for cultures it doesn’t understand.
As someone who’s constantly around AI tools for both creative and academic work, I found this unsettling. These systems are meant to expand creativity, but this article made me see how they can also narrow it, fitting entire cultures into Western centric frames.

## What Stuck With Me...
What stood out most was the concept of *“failure modes.”*

The researchers identified three key ones:

- **Underrepresentation:** South Asian features, clothing, and settings were often absent.
- **Amplifying Western defaults:** Prompts like “a professional woman” produced Eurocentric imagery almost every time.
- **Cultural tropes:** When the models did include South Asian imagery, it leaned into stereotypesm like Bollywood glamour, saris, temples, or poverty.
That last one hit me hard. Growing up, I often saw South Asia portrayed through the same clichés, colorful festivals or chaos, but not the everyday normality I knew. Seeing AI repeat those same visual shortcuts made me realize that bias in data isn’t just a technical flaw, it’s a cultural inheritance.

## I agree, but
I completely agreed with the study’s call for community centered AI design. Representation can’t be “fixed” from the outside, it needs collaboration with the people being represented.

Still, I also felt torn, not because I believe generative AI is a solution, but because I’ve seen how easily it’s framed as one. Companies love to sell it as a tool for creativity and empowerment, but when the systems themselves reproduce bias, that promise feels hollow. Instead of democratizing art, these tools often reinforce who gets to be seen as “artistic” or “modern.” Inclusivity shouldn’t be a marketing checkbox, it should be the foundation, and yet it rarely is.

The article argues that AI is never neutral, and I think that’s true, but I’d also add that neutrality isn’t always the goal. Sometimes, the goal should be intentional diversity: training models to see more of the world, not just mirror dominant data.

## I tried
Using image generators in my own projects, I’ve noticed similar patterns. When I typed prompts like “young woman in traditional dress,” the AI assumed “traditional” meant Western wedding gowns or sometimes hyper sexualized fashion poses.

To get something even remotely culturally accurate, I had to overdescribe: mention ethnicity, geography, even colors. It made me feel like I was negotiating with the algorithm, begging it to see what should’ve been obvious.

That’s when I realized that “bias” isn’t just an academic term. It’s a user experience problem that makes entire identities harder to access.

## The Question I'd Ask 
If I could add one question to the case study, it would be:

*“What would representation look like if local artists, developers, and storytellers from these underrepresented regions were given the same resources to train their own models?”*

I think this question matters because it shifts the focus from correction to creation. Instead of fixing broken representations, what if we empowered new ones?

## Some final thoughts 
This article changed how I think about technology ethics. Before, I saw bias as a technical challenge, something that needed a fix. Now, I see it as a narrative challenge,  ***something we all need to rethink***.

Representation isn’t just about what AI shows us, it’s about whose imaginations it includes.
And as someone who uses these tools daily, I feel a stronger responsibility to ask: Who’s missing from the picture I just created?