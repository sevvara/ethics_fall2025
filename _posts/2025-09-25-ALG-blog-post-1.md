---
title: 'ALG Blog Post 1: Exception to Data Driven Rules'
date: 2025-09-25
permalink: /posts/2025/09/ALG-blog-post-1/
tags:
  - ethics
  - uncertainty 
  - individualization
  - exceptions
  - data-driven rules
---

*Exploring when strict data-driven rules fall short, and why exceptions matter for fairness.* 

**Case Study:** 
["The Right to Be an Exception to a Data‑Driven Rule" by Sarah H. Cen and Manish Raghavan](https://mit-serc.pubpub.org/pub/right-to-be-exception/release/2)  


## Purpose Of The Case Study
The case study looks at data-driven rules and the idea of exceptions. It asks when rules built on data should bend, and why uncertainty sometimes makes exceptions not just useful, but necessary.

## Q1. What is a data‑driven rule, and what does it mean to be a data‑driven exception? Is an exception the same as an error?

A data-driven rule is basically when decisions are made strictly by the numbers: for example, “deny the loan if the credit score is below 650.” An exception is when we recognize that the rigid rule doesn’t capture someone’s full situation. For instance, maybe a person’s credit score dipped because of unexpected medical bills, but otherwise they’re financially stable. That’s not an “error” in the system, it’s the system being too blunt. Exceptions acknowledge that people are more complex than the patterns they fit into.

## Q2. In addition to those listed above, what other factors differentiate data-driven decisions from human ones? 

 Humans bring in empathy, context, and sometimes just gut instinct. A judge or teacher might notice body language, tone, or backstory that an algorithm completely misses. At the same time, humans also carry their own biases, which can make their decisions inconsistent or unfair. Data-driven decisions, on the other hand, are consistent but rigid, they treat everyone the same whether or not it makes sense. It’s like the difference between a GPS and a friend giving you directions: the GPS is exact, but your friend might warn you about construction or suggest a prettier route.

## Q3. Beyond what is discussed above, what are some of the benefits and downsides of individualization?

Individualization feels fairer because it acknowledges people’s unique circumstances. For example, a teacher who gives extra time on an assignment to a student going through a tough situation is recognizing the human side of education. But the downside is that it can look like favoritism or make the system less predictable. If too many exceptions are made, people may stop trusting the rules altogether. Striking a balance is hard, too inflexible feels cold, too flexible feels chaotic.


## Q4. Why is uncertainty so critical to the right to be an exception? When the stakes are high (e.g., in criminal sentencing), is there any evaluation metric (e.g., accuracy) that can justify the use of a data-driven rule without the consideration of uncertainty?

Uncertainty matters because no dataset can perfectly capture human life. If the stakes are high, like in criminal sentencing or medical decisions, blindly following a rule can cause real harm. Even if an algorithm has a 90% “accuracy” rate, that 10% error margin represents real people’s lives. Metrics like accuracy might look good on paper, but they can’t replace the ethical need to pause and ask, “What if this case is one of the outliers?” Exceptions are society’s way of leaving room for doubt and recognizing that human judgment is still necessary.

## New Question
**How should individuals be empowered to contest data‑driven decisions that negatively affect them?**

The case study argues that the burden of proof should shift from the decision subject to the decision maker. Yet in practice, challenging an algorithmic decision requires knowledge and resources. My question invites discussion on mechanisms, transparency requirements, legal rights, advocacy groups, or technical audit tools, that can help people assert their right to be an exception and meaningfully contest an unfair algorithmic decision. This connects with my experience of seeing people accept automated decisions without understanding that they can ask for explanations or human review.

## Reflection
Reading this case study made me think about the hidden costs of convenience. Algorithms promise efficiency, but they also risk codifying statistical averages into decisions that shape people’s lives. I was struck by the argument that being an exception isn’t a personal failing, it’s often a natural result of how models are trained. As someone who benefits from recommender systems and automated tools, I realise that I rarely think about the people for whom those tools fail. This exercise challenged me to consider when it is appropriate to rely on an algorithm and when we should slow down, acknowledge uncertainty and treat people as individuals.
